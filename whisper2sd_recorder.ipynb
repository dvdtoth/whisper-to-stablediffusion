{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvdtoth/whisper-to-stablediffusion/blob/main/whisper2sd_recorder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper to Stable Diffusion\n",
        "\n",
        "Two open-source models hacked together for a cool speech2image function.\n",
        "\n",
        "Using the recently released speech recognition model, Whisper from openai.com,\n",
        "and the text to image GAN, Stable Diffusion from stability.ai.\n",
        "\n",
        "Requires a https://huggingface.co/ account.\n",
        "\n",
        "\n",
        "### Sources\n",
        "\n",
        "Models:\n",
        " * https://github.com/openai/whisper\n",
        " * https://github.com/CompVis/stable-diffusion\n",
        "\n",
        "Whisper to SD:\n",
        " * https://huggingface.co/spaces/fffiloni/whisper-to-stable-diffusion\n",
        " * https://colab.research.google.com/drive/12DzxLRpCEDE7OtKMlkAtHKbNVGBlk7Pz\n",
        "\n",
        "\n",
        "\n",
        "@dvdtoth"
      ],
      "metadata": {
        "id": "A9omS-pJooEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJOpPqXyL7xa"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi #Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prep environment\n",
        "!pip install diffusers==0.3.0\n",
        "!pip install transformers scipy ftfy ipywebrtc\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!pip install git+https://github.com/openai/whisper.git \n",
        "!apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "Zuzp6ALiMJBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to huggingface\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "QVFVVUW_M28L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from torch import autocast\n",
        "import whisper\n",
        "import IPython.display as ipd\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True)  \n",
        "pipe = pipe.to(\"cuda\")\n",
        "whisper_model = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "x4hq_teoM3xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record audio\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "\n",
        "audio = CameraStream(constraints={'audio': True, 'video': False})\n",
        "recorder = AudioRecorder(stream=audio)\n",
        "recorder"
      ],
      "metadata": {
        "id": "O-b7-ltP-azK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recognise and translate prompt\n",
        "audio_file_path = '/content/audio.webm'\n",
        "recorder.save(audio_file_path)\n",
        "\n",
        "audio = whisper.load_audio(audio_file_path)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "    \n",
        "mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
        "    \n",
        "_, probs = whisper_model.detect_language(mel)\n",
        "    \n",
        "options = whisper.DecodingOptions(task=\"translate\", fp16 = False)\n",
        "prompt = whisper.decode(whisper_model, mel, options)\n",
        "print(\"Translated prompt:\", prompt.text)"
      ],
      "metadata": {
        "id": "f25CkMEePANK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate image\n",
        "with autocast(\"cuda\"):\n",
        "  image = pipe(prompt.text).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "9qiPycDGXR_m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}